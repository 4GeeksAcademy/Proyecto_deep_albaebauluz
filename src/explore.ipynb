{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "#!pip install matplotlib\n",
    "#!pip install opencv-python\n",
    "#!pip install pillow\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Definir las rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = '/workspaces/Proyecto_deep_albaebauluz/data/train.zip'   # Ruta del archivo zip\n",
    "extract_path = '/workspaces/Proyecto_deep_albaebauluz/data/train'    # Carpeta donde descomprimiremos las imágenes\n",
    "base_dir = '/workspaces/Proyecto_deep_albaebauluz/data/organized_data'  # Carpeta donde organizaremos el conjunto de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Descomprimir el archivo zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "print(\"Descompresión completa.\")\n",
    "\n",
    "# Revisar el contenido de la carpeta después de la descompresión\n",
    "extract_path = '/workspaces/Proyecto_deep_albaebauluz/data/train'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Obtener la lista de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = os.listdir(extract_path)\n",
    "print(\"Número de imágenes encontradas:\", len(all_images))\n",
    "print(\"Primeros 10 nombres de archivos:\", all_images[:10])  # Muestra los primeros 10 nombres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Filtrar las listas de perros y gatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_images = [img for img in all_images if 'dog' in img]\n",
    "cat_images = [img for img in all_images if 'cat' in img]\n",
    "\n",
    "print(\"Número de imágenes de perros:\", len(dog_images))\n",
    "print(\"Número de imágenes de gatos:\", len(cat_images))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Actualizar la ruta después de verificar la estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_path = '/workspaces/Proyecto_deep_albaebauluz/data/train/train'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Obtener la lista de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = os.listdir(extract_path)\n",
    "print(\"Número de imágenes encontradas:\", len(all_images))\n",
    "print(\"Primeros 10 nombres de archivos:\", all_images[:10])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Filtrar las listas de perros y gatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_images = [img for img in all_images if 'dog' in img]\n",
    "cat_images = [img for img in all_images if 'cat' in img]\n",
    "\n",
    "print(\"Número de imágenes de perros:\", len(dog_images))\n",
    "print(\"Número de imágenes de gatos:\", len(cat_images))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Definir la carpeta base para organizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/workspaces/Proyecto_deep_albaebauluz/data/organized_data'  # Carpeta donde organizo el conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- Dividir en entrenamiento y validación (80% para entrenamiento y 20% para validación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dogs, val_dogs = train_test_split(dog_images, test_size=0.2, random_state=42)\n",
    "train_cats, val_cats = train_test_split(cat_images, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II- Crear carpetas de destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(base_dir, 'train/dog'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'train/cat'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'val/dog'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'val/cat'), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III- Mover archivos a las carpetas correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(file_list, source_dir, target_dir):\n",
    "    for file_name in file_list:\n",
    "        shutil.move(os.path.join(source_dir, file_name), os.path.join(target_dir, file_name))\n",
    "\n",
    "\n",
    "# Mover imágenes de perros y gatos a las carpetas correspondientes\n",
    "move_files(train_dogs, extract_path, os.path.join(base_dir, 'train/dog'))\n",
    "move_files(val_dogs, extract_path, os.path.join(base_dir, 'val/dog'))\n",
    "move_files(train_cats, extract_path, os.path.join(base_dir, 'train/cat'))\n",
    "move_files(val_cats, extract_path, os.path.join(base_dir, 'val/cat'))\n",
    "\n",
    "print(\"Organización completa. Las imágenes están listas para el entrenamiento y la validación.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visualiza la información de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizar las primeras nueve imágenes de perros y gatos\n",
    "def plot_images(images_folder, label, num_images=9):\n",
    "    images = [img for img in os.listdir(images_folder) if label in img][:num_images]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, img_name in enumerate(images):\n",
    "        img_path = os.path.join(images_folder, img_name)\n",
    "        img = Image.open(img_path)  # Cargar imagen con Pillow\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas para las imágenes de perros y gatos\n",
    "dog_images_folder = '/workspaces/Proyecto_deep_albaebauluz/data/train/train'  \n",
    "cat_images_folder = '/workspaces/Proyecto_deep_albaebauluz/data/train/train'  \n",
    "\n",
    "print(\"Primeras nueve imágenes de perros:\")\n",
    "plot_images(dog_images_folder, 'dog')\n",
    "\n",
    "print(\"Primeras nueve imágenes de gatos:\")\n",
    "plot_images(cat_images_folder, 'cat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Configurar los generadores de datos para entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Crear los flujos de datos para entrenamiento y validación desde las carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/workspaces/Proyecto_deep_albaebauluz/data/organized_data/train'\n",
    "val_dir = '/workspaces/Proyecto_deep_albaebauluz/data/organized_data/val'\n",
    "\n",
    "trdata = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "tsdata = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener un lote de datos de entrenamiento\n",
    "images, labels = next(trdata)\n",
    "\n",
    "# Mostrar las primeras 9 imágenes\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(\"Dog\" if labels[i] == 1 else \"Cat\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Modelo simplificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(128,128,3)))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen del modelo para confirmar la arquitectura\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir early stopping para evitar sobreajuste\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    trdata,\n",
    "    validation_data=tsdata,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo en los datos de validación\n",
    "loss, accuracy = model.evaluate(tsdata)\n",
    "print(f\"Precisión en el conjunto de validación: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Optimización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"best_model.keras\",         # Cambio la extensión a .keras\n",
    "    monitor=\"val_accuracy\",     # Métrica para monitorear\n",
    "    save_best_only=True,        # Guardo el mejor modelo\n",
    "    mode=\"max\",                 # Guardo el modelo con la precisión de validación más alta\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",         # Monitorea la pérdida en el conjunto de validación\n",
    "    patience=5,                 # Detiene el entrenamiento si no hay mejora en 5 épocas\n",
    "    restore_best_weights=True   # Restaura el mejor peso al final\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo con los callbacks\n",
    "history = model.fit(\n",
    "    trdata,\n",
    "    validation_data=tsdata,\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el mejor modelo guardado\n",
    "best_model = load_model(\"best_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = best_model.evaluate(tsdata)\n",
    "print(f\"Precisión en el conjunto de prueba: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "predicciones = best_model.predict(tsdata)\n",
    "predicciones_clases = (predicciones > 0.5).astype(\"int32\")  # Umbral para convertir probabilidades en clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los resultados\n",
    "print(\"Predicciones de las primeras 10 imágenes de prueba:\", predicciones_clases[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la carpeta 'models' \n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Guardo el modelo en la carpeta 'models'\n",
    "model_path = '/workspaces/Proyecto_deep_albaebauluz/models/best_model.keras'\n",
    "best_model.save(model_path)\n",
    "\n",
    "print(f\"Modelo guardado en: {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: Tras realizar todo el ejercicio, no me deja guardar los resultados porque pesa mucho los datos. He intentado solucionar el problema, pero al no disponer de la plataforma local me ha sido imposible solventarlo.\n",
    "\n",
    "Enumerating objects: 25037, done.\n",
    "Counting objects: 100% (25037/25037), done.\n",
    "Delta compression using up to 2 threads\n",
    "Compressing objects: 100% (25029/25029), done.\n",
    "Writing objects: 100% (25032/25032), 1.22 GiB | 36.53 MiB/s, done.\n",
    "Total 25032 (delta 16), reused 25006 (delta 1), pack-reused 0\n",
    "remote: Resolving deltas: 100% (16/16), completed with 1 local object.\n",
    "remote: error: Trace: bf389b2be3ac8a7631ac3d7f862016c9f1c7dd22a22c903a5ba074ae0bcb00cc\n",
    "remote: error: See https://gh.io/lfs for more information.\n",
    "remote: error: File src/best_model.keras is 193.12 MB; this exceeds GitHub's file size limit of 100.00 MB\n",
    "remote: error: File data/train.zip is 543.16 MB; this exceeds GitHub's file size limit of 100.00 MB\n",
    "remote: error: File models/best_model.keras is 193.12 MB; this exceeds GitHub's file size limit of 100.00 MB\n",
    "remote: error: File src/models/best_model.keras is 193.12 MB; this exceeds GitHub's file size limit of 100.00 MB\n",
    "remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\n",
    "To https://github.com/4GeeksAcademy/Proyecto_deep_albaebauluz\n",
    " ! [remote rejected]   main -> main (pre-receive hook declined)\n",
    "error: failed to push some refs to 'https://github.com/4GeeksAcademy/Proyecto_deep_albaebauluz'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
